accelerate==1.6.0
annotated-types==0.7.0
asttokens==3.0.0
certifi==2024.12.14
charset-normalizer==3.4.1
colorama==0.4.6
decord==0.6.0
deepspeed==0.16.2
einops==0.8.0
einops-exts==0.0.4
exceptiongroup==1.2.2
executing==2.1.0
filelock==3.17.0
flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.9.post1/flash_attn-2.5.9.post1+cu122torch2.4cxx11abiFALSE-cp39-cp39-linux_x86_64.whl#sha256=60bb4876982cf53eab1f2fe67da556862c87e35d36732abc06a87e3e6500a145
fsspec==2024.12.0
ftfy==6.3.1
hjson==3.1.0
huggingface-hub==0.27.1
icecream==2.1.4
idna==3.10
iniconfig==2.0.0
Jinja2==3.1.5
liger_kernel==0.5.2
MarkupSafe==3.0.2
mpmath==1.3.0
msgpack==1.1.0
nersc-pymon==0.2.1
networkx==3.2.1
ninja==1.11.1.3
numpy==2.0.2
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu12==2.20.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.1.105
open-clip-torch==2.16.0
opencv-python==4.11.0.86
packaging==24.2
peft==0.15.2
pillow==11.1.0
pluggy==1.5.0
protobuf==3.20.3
psutil==6.1.1
py-cpuinfo==9.0.0
pydantic==2.10.6
pydantic_core==2.27.2
Pygments==2.19.1
pytest==8.3.4
PyYAML==6.0.2
regex==2024.11.6
requests==2.32.3
safetensors==0.5.2
sentencepiece==0.2.0
sympy==1.13.1
timm==1.0.14
tokenizers==0.21.0
tomli==2.2.1
torch==2.4.0
torchvision==0.19.0
tqdm==4.67.1
transformers==4.47.1
triton==3.0.0
typing_extensions==4.12.2
urllib3==2.3.0
wcwidth==0.2.13
xformers==0.0.27.post2
